{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework week08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 [Speculative Sampling][4 points]\n",
    "\n",
    "Algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model.\n",
    "\n",
    "\n",
    "Carefully read https://arxiv.org/abs/2302.01318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"654\" alt=\"image\" src=\"https://github.com/markovka17/dla/assets/20357655/db624e40-d4f0-4e36-88e7-b58a6c646738\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take `EleutherAI/gpt-neo-1.3B` LM as a draft model from https://huggingface.co and generate a couple dozen tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that your device is set correctly (GPU or CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B', torch_dtype=torch.float16).to('cuda')\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Prepare a text prompt\n",
    "text_prompt = [\"The quick brown fox jumps\"]\n",
    "inputs = tokenizer(text_prompt, return_tensors='pt').to(device)  # Tokenize the text prompt and convert to tensor\n",
    "\n",
    "# Perform text generation (inference)\n",
    "# Note: manual handling means we will manage the generated text and stop criteria without using generate() method\n",
    "max_length = 100  # Maximum length of the generated text\n",
    "temperature = 1.0  # Sampling temperature, higher values mean more randomness\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    output_sequence = inputs['input_ids']\n",
    "    for _ in range(max_length - inputs['input_ids'].size(1)):\n",
    "        # Predict the next token\n",
    "        logits = model(output_sequence).logits[:, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Sample the next token from the probability distribution\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        \n",
    "        # Append the predicted token to the output sequence\n",
    "        output_sequence = torch.cat([output_sequence, next_token], dim=1)\n",
    "\n",
    "        # Check if the end-of-sequence token (EOS) was generated\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output_sequence.squeeze(), skip_special_tokens=True)\n",
    "print(\"Generated text:\\n\", generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1\n",
    "\n",
    "Ans let's use `EleutherAI/gpt-j-6B` as target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In speculative sampling, we have two models:\n",
    "\n",
    "    A smaller, faster draft model (e.g. EleutherAI/gpt-neo-1.3B model)\n",
    "    A larger, slower target model (e.g. EleutherAI/gpt-j-6B model)\n",
    "\n",
    "The idea is that the draft model speculates what the output is steps into the future, while the target model determines how many of those tokens we should accept. Here's an outline of the algorithm:\n",
    "\n",
    "The draft model decodes tokens in the regular autoregressive fashion.\n",
    "We get the probability outputs of the target and draft model on the new predicted sequence.\n",
    "We compare the target and draft model probabilities to determine how many of the tokens we want to keep based on some rejection criteria. If a token is rejected, we resample it using a combination of the two distributions and don't accept any more tokens.\n",
    "If all tokens are accepted, we can sample an additional final token from the target model probability output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"635\" alt=\"image\" src=\"https://github.com/markovka17/dla/assets/20357655/3954894d-8735-4f92-a835-d04eac74f190\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The backbone of the speculative sample. Feel free to modify it\n",
    "\n",
    "def speculative_sampling(x, draft_model, target_model, N, K):\n",
    "    # NOTE: paper indexes arrays starting from 1, python indexes from 0, so\n",
    "    # we have to add an extra -1 term when indexing using n, T, or t\n",
    "    n = len(x)\n",
    "\n",
    "    for _ in range(N):\n",
    "        # Step 1: auto-regressive decode K tokens from draft model and get final p\n",
    "        x_draft = x\n",
    "        for _ in range(K):\n",
    "            pass\n",
    "            # TODO\n",
    "\n",
    "        # Step 2: target model forward passes on x_draft\n",
    "        # TODO\n",
    "\n",
    "        # Step 3: append draft tokens based on rejection criterion and resample\n",
    "        # a token on rejection\n",
    "        all_accepted = True\n",
    "        for _ in range(K):\n",
    "            pass\n",
    "            # TODO\n",
    "\n",
    "        # Step 4: if all draft tokens were accepted, sample a final token\n",
    "        if all_accepted:\n",
    "            pass\n",
    "            # TODO\n",
    "            \n",
    "\n",
    "        # just keeping my sanity\n",
    "        assert n == len(x), f\"{n} {len(x)}\"\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2\n",
    "\n",
    "Compare the speed of SpS with ArS. The expected speed increase is 30-50%. \n",
    "The speedup is equal to `(time spent by ArS)` / `(time spend by SpS)`\n",
    "\n",
    "Use same start prompt `The quick brown fox jumps`, `K=16` and `K=32` (compare two scenarios) and `max_length=512`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3\n",
    "\n",
    "Visualise acceptence rate for `K=[16, 32, 64, 128]`, same start prompt and `max_length max_length=1024`, where graft model is `EleutherAI/gpt-neo-1.3B` and target model if `EleutherAI/gpt-j-6B`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 [GroupNorm in Triton][6 points]\n",
    "\n",
    "\n",
    "You need to implement a 2D GroupNorm (https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html) and compare it to the PyTorch implementation.\n",
    "Note that GroupNorm is very similar to LayerNorm, so you can see the LayerNorm implementation here https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _group_norm_fwd_fused(\n",
    "    X,  # pointer to the input\n",
    "    Y,  # pointer to the output\n",
    "    W,  # pointer to the weights\n",
    "    B,  # pointer to the biases\n",
    "    Mean,  # pointer to the mean\n",
    "    Rstd,  # pointer to the 1/std\n",
    "    stride,  # how much to increase the pointer when moving by 1 row\n",
    "    N,  # number of columns in X\n",
    "    num_groups,  # number of groups\n",
    "    eps,  # epsilon to avoid division by zero\n",
    "    BLOCK_SIZE: tl.constexpr,  # Same parameters as in matmul from seminar\n",
    "):\n",
    "    \"\"\"\n",
    "    Similar to forward of nn.GroupNorm.forward\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _group_norm_bwd_dx_fused(\n",
    "    DX,  # pointer to the input gradient\n",
    "    DY,  # pointer to the output gradient\n",
    "    DW,  # pointer to the partial sum of weights gradient\n",
    "    DB,  # pointer to the partial sum of biases gradient\n",
    "    X,  # pointer to the input\n",
    "    W,  # pointer to the weights\n",
    "    B,  # pointer to the biases\n",
    "    Mean,  # pointer to the mean\n",
    "    Rstd,  # pointer to the 1/std\n",
    "    stride,  # how much to increase the pointer when moving by 1 row\n",
    "    N,  # number of columns in X\n",
    "    eps,  # epsilon to avoid division by zero\n",
    "    GROUP_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr  # Same parameters as in matmul from seminar\n",
    "):\n",
    "    \"\"\"\n",
    "    Backward of GroupNorm respect to input\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _group_norm_bwd_dwdb(\n",
    "    DW,  # pointer to the partial sum of weights gradient\n",
    "    DB,  # pointer to the partial sum of biases gradient\n",
    "    FINAL_DW,  # pointer to the weights gradient\n",
    "    FINAL_DB,  # pointer to the biases gradient\n",
    "    M,  # GROUP_SIZE_M\n",
    "    N,  # number of columns\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr  # Same parameters as in matmul from seminar\n",
    "):\n",
    "    \"\"\"\n",
    "    Backward of GroupNorm respect to weights and biases (affine transform parameters)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx):\n",
    "        pass\n",
    "\n",
    "group_norm = GroupNorm.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_group_norm(input_shape, num_groups, dtype, eps=1e-5, device='cuda'):\n",
    "    # create data\n",
    "    B, C, H, W = input_shape\n",
    "    weight = torch.rand(C, dtype=dtype, device='cuda', requires_grad=True)\n",
    "    bias = torch.rand(C, dtype=dtype, device='cuda', requires_grad=True)\n",
    "    x = -2.3 + 0.5 * torch.randn(input_shape, dtype=dtype, device='cuda')\n",
    "    dy = .1 * torch.randn_like(x)\n",
    "\n",
    "    x.requires_grad_(True)\n",
    "    # forward pass\n",
    "    y_tri = group_norm(x, num_groups, weight, bias, eps)\n",
    "    y_ref = torch.nn.functional.group_norm(x, num_groups, weight, bias, eps).to(dtype)\n",
    "    # backward pass (triton)\n",
    "    y_tri.backward(dy, retain_graph=True)\n",
    "    dx_tri, dw_tri, db_tri = [_.grad.clone() for _ in [x, weight, bias]]\n",
    "    x.grad, weight.grad, bias.grad = None, None, None\n",
    "    # backward pass (torch)\n",
    "    y_ref.backward(dy, retain_graph=True)\n",
    "    dx_ref, dw_ref, db_ref = [_.grad.clone() for _ in [x, weight, bias]]\n",
    "    # compare\n",
    "    assert torch.allclose(y_tri, y_ref, atol=1e-2, rtol=0)\n",
    "    assert torch.allclose(dx_tri, dx_ref, atol=1e-2, rtol=0)\n",
    "    assert torch.allclose(db_tri, db_ref, atol=1e-2, rtol=0)\n",
    "    assert torch.allclose(dw_tri, dw_ref, atol=1e-2, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 32, 64, 64)\n",
    "num_groups = 8\n",
    "test_group_norm(input_shape, num_groups, torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1\n",
    "\n",
    "Visualize perfomance benchmark using `triton.testing.perf_report`. Similar to matmul benchmark from seminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to check your implementation for the following parameters\n",
    "batch_size = 2\n",
    "for image_resolution in [32, 128, 512, 1024, 1536, 2048]:\n",
    "    for num_channels in [32, 128, 386, 512]:\n",
    "        for num_groups in [1, 4, 8, 16, 32]:\n",
    "\n",
    "            dummy_input = torch.randn(batch_size, num_channels, image_resolution, image_resolution)\n",
    "\n",
    "            # TODO benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
