{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, Dict, Sequence\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm import trange\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.amp import autocast\n",
    "import torch.backends.xnnpack\n",
    "from torch.profiler import profile, schedule, ProfilerActivity\n",
    "\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy profiling tool\n",
    "\n",
    "This will be our main utility for analyzing performance on the GPU.\n",
    "\n",
    "Pay attention to the scheduler variable in the config. It is very important to make several idle runs before the active phase of measurement to warm up GPU caches\n",
    "\n",
    "We will focus on the last line \"Self CUDA time total: ...\", but in general we can analyze specific operations in the table as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProfilerConfig:\n",
    "    scheduler: Optional[Callable[[int], int]] = field(default=None, metadata={\"omegaconf_ignore\": True})\n",
    "    num_steps: int = field(init=False)\n",
    "    amp: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.scheduler is None:\n",
    "            self.scheduler = {\"skip_first\": 2, \"wait\": 2, \"warmup\": 5, \"active\": 3, \"repeat\": 0}\n",
    "\n",
    "        self.num_steps = sum([v for k, v in self.scheduler.items() if k != \"repeat\"])\n",
    "        self.scheduler = schedule(**self.scheduler)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def profile_model(model: nn.Module, input: Dict[str, torch.Tensor], config: ProfilerConfig):\n",
    "    training_state = model.training\n",
    "    model.eval()\n",
    "\n",
    "    with autocast(dtype=torch.float16, device_type=\"cuda\", enabled=config.amp), profile(\n",
    "        activities=[ProfilerActivity.CUDA ],\n",
    "        schedule=config.scheduler\n",
    "    ) as p:\n",
    "        for _ in range(config.num_steps):\n",
    "            _ = model(**input)\n",
    "            p.step()\n",
    "\n",
    "    model.train(training_state)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(100, 10000).cuda()\n",
    "\n",
    "p = profile_model(m, {'input': torch.randn(1024, 100).cuda()}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusing\n",
    "\n",
    "Assume we have HUGE transformer based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projections Fusing\n",
    "\n",
    "This technique aims to reduce the number of operations. Instead of 3 matrix multiplications (projections on q, k and v) \n",
    "we will have one big one, due to which we will reduce the number of calls and memory moves to GPU registers.\n",
    "\n",
    "The main work will take place in the `fuse_qkv` function, which should be called once before using the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummySelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert self.head_dim * heads == embed_size, \"Embed size needs to be divisible by heads\"\n",
    "\n",
    "        # Linear layers to generate Query, Key, and Value matrices\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "        self.fused_qkv_projection = None\n",
    "\n",
    "    def fuse_qkv(self):\n",
    "        @torch.no_grad()\n",
    "        def fuse_projections(*projections: Sequence[nn.Linear]) -> nn.Linear:\n",
    "            concatenated_weights = torch.cat([p.weight.data for p in projections])\n",
    "            device = concatenated_weights.device\n",
    "            dtype = concatenated_weights.dtype\n",
    "            in_features = concatenated_weights.shape[1]\n",
    "            out_features = concatenated_weights.shape[0]\n",
    "            projection = nn.Linear(in_features, out_features, bias=False, device=device, dtype=dtype)\n",
    "            projection.weight.copy_(concatenated_weights)\n",
    "            return projection\n",
    "        \n",
    "        self.fused_qkv_projection = fuse_projections(self.queries, self.keys, self.values)\n",
    "        del self.queries, self.keys, self.values\n",
    "\n",
    "    def forward(self, input: torch.Tensor, kv_cache=None) -> torch.Tensor:\n",
    "        batch_size = input.shape[0]\n",
    "        input_len = input.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads pieces\n",
    "        input = input.reshape(batch_size, input_len, self.heads, self.head_dim)\n",
    "\n",
    "        if self.fused_qkv_projection is None:\n",
    "            q = self.queries(input)\n",
    "            k = self.keys(input)\n",
    "            v = self.values(input)\n",
    "\n",
    "        else:\n",
    "            q, k, v = self.fused_qkv_projection(input).chunk(chunks=3, dim=-1)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [q, k])\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, v]).reshape(batch_size, input_len, self.heads*self.head_dim)\n",
    "        return self.fc_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = DummySelfAttention(1024, 8).cuda()\n",
    "sa_input = torch.randn(32, 128, 1024).cuda()\n",
    "\n",
    "p = profile_model(sa, {'input': sa_input}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_fused = DummySelfAttention(1024, 8).cuda()\n",
    "sa_fused.load_state_dict(sa.state_dict())\n",
    "sa_fused.fuse_qkv()\n",
    "\n",
    "p = profile_model(sa_fused, {'input': sa_input}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(sa(sa_input), sa_fused(sa_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the result of work with and without fusing is identical, but at the same time we see a 7-10% increase in timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large scale\n",
    "\n",
    "For larger nets, the absolute value of acceleration will be larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_large = nn.Sequential(*[DummySelfAttention(1024, 8)] * 10).cuda()\n",
    "\n",
    "p = profile_model(sa_large, {'input': sa_input}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_fused_large = nn.Sequential(*[DummySelfAttention(1024, 8)] * 10).cuda()\n",
    "for m in sa_fused_large.modules():\n",
    "    if isinstance(m, DummySelfAttention):\n",
    "        m.fuse_qkv()\n",
    "\n",
    "p = profile_model(sa_fused_large, {'input': sa_input}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise Fusing\n",
    "\n",
    "The following optimization is similar to the previous one. In modern DL there are many activations, which is a composition of simple operations, e.g. elementwise multiplication/addition, sigmoid/tangent.\n",
    "\n",
    "In case we naively implement such activations, each elementwise operation is performed by a separate kernel. That is, we load memory into it, perform the operation and return the memory back.\n",
    "Such memory management is very slow, so it is much better to perform all operations in a single kernel\n",
    "\n",
    "This fusion of operations can be achieved with the help of decorator `@torch.compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def fused_geglu(x, gate):\n",
    "    \"\"\"\n",
    "    Gaussian error Gated Linear Units, GeGLU(x) = GeLU(x) * gate\n",
    "    from the paper https://arxiv.org/abs/2002.05202\n",
    "    \"\"\"\n",
    "    tanh = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n",
    "    return gate * (x * 0.5 * (1.0 + tanh))\n",
    "\n",
    "\n",
    "class _FusedGeGLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor, gate: torch.Tensor):\n",
    "        ctx.save_for_backward(x, gate)\n",
    "        return fused_geglu(x, gate)\n",
    "    \n",
    "    # def backward()\n",
    "    \n",
    "\n",
    "def geglu(x, gate):\n",
    "    \"\"\"\n",
    "    Gaussian error Gated Linear Units, GeGLU(x) = GeLU(x) * gate\n",
    "    from the paper https://arxiv.org/abs/2002.05202\n",
    "    \"\"\"\n",
    "    tanh = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n",
    "    return gate * (x * 0.5 * (1.0 + tanh))\n",
    "\n",
    "class _GeGLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x: torch.Tensor, gate: torch.Tensor):\n",
    "        ctx.save_for_backward(x, gate)\n",
    "        return geglu(x, gate)\n",
    "\n",
    "\n",
    "class GeGLU(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int, impl: str):\n",
    "        super().__init__()\n",
    "        # Combined linear projections $xW + b$ and $xV + c$\n",
    "        self.proj = nn.Linear(d_in, d_out * 2)\n",
    "        self._impl= impl\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        x, gate = self.proj(input).chunk(2, dim=-1)\n",
    "        if self._impl == \"naive\":\n",
    "            return _GeGLU.apply(x, gate)\n",
    "        return _FusedGeGLU.apply(x, gate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = GeGLU(2048, 2048, impl=\"naive\").cuda()\n",
    "act_input = torch.randn(32, 128, 2048).cuda()\n",
    "\n",
    "p = profile_model(act, {'input': act_input}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_fused = GeGLU(2048, 2048, impl=\"fast\").cuda()\n",
    "act_fused.load_state_dict(act.state_dict())\n",
    "\n",
    "p = profile_model(act_fused, {'input': act_input}, ProfilerConfig(amp=False))\n",
    "print(p.key_averages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(act(act_input), act_fused(act_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this method gives a gain of almost two times in speed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KV cache\n",
    "\n",
    "The KV-cache method in language models, specifically in the context of attention mechanisms like those used in Transformer models, refers to a technique where Key (K) and Value (V) vectors from previous attention operations are cached (stored) for reuse in subsequent operations. This method is primarily used to enhance efficiency and speed in autoregressive models, where the prediction of the next token in a sequence depends on the previously generated tokens.\n",
    "\n",
    "By caching the Key and Value pairs from earlier tokens, the model avoids redundant computations for these tokens when processing new tokens in the sequence. This is particularly beneficial in tasks like text generation, where each new token prediction requires considering the entire preceding context. The KV-cache method thus enables faster and more computationally efficient inference, making real-time applications and longer sequence generation more feasible.\n",
    "\n",
    "For more details follow the link https://mett29.github.io/posts/kv-cache/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"716\" alt=\"image\" src=\"https://github.com/markovka17/dla/assets/20357655/f51e8cc7-ff13-4df1-88ae-23414e09412f\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super().__init__()\n",
    "        max_batch_size = 32\n",
    "        max_seq_len = 512\n",
    "\n",
    "        # Indicates the number of heads for the Keys and Values\n",
    "        self.n_kv_heads = heads\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.n_heads_q = heads\n",
    "        # Indicates how many times the Keys and Values should be repeated\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.wq = nn.Linear(embed_size, self.n_heads_q * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(embed_size, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(embed_size, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(heads * self.head_dim, embed_size, bias=False)\n",
    "\n",
    "        cache_k = torch.zeros((max_batch_size, max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        cache_v = torch.zeros((max_batch_size, max_seq_len, self.n_kv_heads, self.head_dim))\n",
    "        self.register_buffer(\"cache_k\", cache_k)\n",
    "        self.register_buffer(\"cache_v\", cache_v)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int = None, use_cache: bool = False):\n",
    "        batch_size, seq_len, _ = x.shape  # (B, 1, Dim)\n",
    "        xq = self.wq(x)  # (B, 1, Dim) -> (B, 1, H_Q * Head_Dim)\n",
    "        xk = self.wk(x)  # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
    "        xv = self.wv(x)  # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
    "\n",
    "        # print(f\"{xq.shape=} {xk.shape=} {xv.shape=}\")\n",
    "\n",
    "        # (B, 1, H_Q * Head_Dim) -> (B, 1, H_Q, Head_Dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim)\n",
    "        if use_cache:\n",
    "            # Replace the entry in the cache\n",
    "            self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
    "            self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
    "\n",
    "            # All context\n",
    "            keys = self.cache_k[:batch_size, : start_pos + seq_len]\n",
    "            values = self.cache_v[:batch_size, : start_pos + seq_len]\n",
    "        else:\n",
    "            keys = xk\n",
    "            values = xv\n",
    "\n",
    "        # (B, 1, H_Q, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # print(f\"{xq.shape=} {keys.shape=} {values.shape=}\")\n",
    "\n",
    "        # print(f'q_{xq.shape} x k_{keys.transpose(2, 3).shape}')\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)\n",
    "        output = (output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
    "        return self.wo(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(1024, 8).cuda()\n",
    "\n",
    "num_tokens = 512\n",
    "use_kv_cache = True\n",
    "time_per_step = []\n",
    "\n",
    "\n",
    "def generate(num_tokens, use_kv_cache):\n",
    "    time_per_step = []\n",
    "    current_result = torch.randn(32, num_tokens, 1024).cuda()\n",
    "\n",
    "    for token_index in trange(num_tokens):\n",
    "        start_t = time.monotonic()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            if use_kv_cache:\n",
    "                res = sa(current_result[:, token_index : token_index + 1 ], start_pos=token_index, use_cache=True)\n",
    "                current_result[:, token_index + 1:token_index + 2] = res\n",
    "            else:\n",
    "                res = sa(current_result[:, :token_index + 1], use_cache=False)\n",
    "                current_result[:, token_index + 1:token_index + 2] = res[:, -1].unsqueeze(dim=1)\n",
    "                \n",
    "        ellapsed_t = time.monotonic() - start_t\n",
    "        time_per_step.append(ellapsed_t)\n",
    "    return time_per_step\n",
    "\n",
    "total_time_with_kv_cache = generate(num_tokens, use_kv_cache=True)\n",
    "total_time_without_kv_cache = generate(num_tokens, use_kv_cache=False)\n",
    "\n",
    "plt.plot(total_time_with_kv_cache, label=f\"with kv-cache | {sum(total_time_with_kv_cache):.3f}\")\n",
    "plt.plot(total_time_without_kv_cache, label=f\"without kv-cache | {sum(total_time_without_kv_cache):.3f}\")\n",
    "plt.xlabel(\"Num tokens\")\n",
    "plt.ylabel(\"Seconds\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see starting from which token naive generation slows down a lot. This becomes especially important when we have a large context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton\n",
    "\n",
    "The aim of the Triton framework, developed by OpenAI, is to simplify and optimize the development of highly efficient GPU-based computations for deep learning models. It provides a Python-like programming environment that allows researchers and developers to write highly parallel and performant custom operations more easily than traditional GPU programming approaches, such as CUDA. Triton aims to make it more accessible for developers to leverage GPU acceleration, ultimately facilitating the development of faster and more efficient deep learning models.\n",
    "\n",
    "The examples are taken from https://triton-lang.org/main/index.html, so feel free to read the documentation yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def add_kernel(\n",
    "    x_ptr,  # *Pointer* to first input vector.\n",
    "    y_ptr,  # *Pointer* to second input vector.\n",
    "    output_ptr,  # *Pointer* to output vector.\n",
    "\n",
    "    n_elements,  # Size of the vector.\n",
    "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "    # NOTE: `constexpr` so it can be used as a shape value.\n",
    "):\n",
    "    # There are multiple 'programs' processing different data. We identify which program\n",
    "    # we are here:\n",
    "\n",
    "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
    "\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # Note that offsets is a list of pointers:\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # NOTE\n",
    "    # tl.device_print('arange', tl.arange(0, BLOCK_SIZE))\n",
    "\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n",
    "    # multiple of the block size.\n",
    "\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "\n",
    "    # Write x + y back to DRAM.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    # We need to preallocate the output.\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
    "    n_elements = output.numel()\n",
    "\n",
    "    # The SPMD (single program, multiple data) launch grid denotes the number of kernel instances that run in parallel.\n",
    "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "\n",
    "    # NOTE:\n",
    "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
    "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n",
    "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
    "\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    \n",
    "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n",
    "    # running asynchronously at this point.\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "size = 98432\n",
    "x = torch.rand(size, device='cuda')\n",
    "y = torch.rand(size, device='cuda')\n",
    "output_torch = x + y\n",
    "output_triton = add(x, y)\n",
    "print(output_torch)\n",
    "print(output_triton)\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a stride in memory formats?\n",
    "\n",
    "In memory formats, a stride is a term that describes the step size or the distance (measured in elements or bytes) between consecutive elements of a data structure (such as an array or a tensor) along a particular dimension when laid out in memory. Strides are crucial for efficiently accessing multi-dimensional data structures, especially when dealing with operations that require reshaping, slicing, or broadcasting of data without physically rearranging it in memory.\n",
    "\n",
    "For instance, consider a 2D array (matrix) stored in row-major order (common in C and Python numpy arrays), where all elements of a row are stored in contiguous memory locations. The stride along the row dimension (often called the \"row stride\") would be 1, indicating that elements along a row are adjacent in memory. The stride along the column dimension (the \"column stride\"), however, would be equal to the number of columns in the array, as one has to skip all elements of a row to move to the next element in a column.\n",
    "\n",
    "By manipulating strides, software libraries can implement operations like transposition, slicing, and various tensor manipulations efficiently, allowing for complex data manipulations without the need for expensive memory copies. Strides are a fundamental concept in libraries that handle multi-dimensional data structures, ensuring data is accessed and manipulated efficiently in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://pytorch.org/assets/images/tensor/image1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.xnnpack\n",
    "\n",
    "print(\"XNNPACK is enabled: \", torch.backends.xnnpack.enabled, \"\\n\")\n",
    "\n",
    "N, C, H, W = 2, 3, 200, 200\n",
    "x = torch.rand(N, C, H, W)\n",
    "print(\"Contiguous shape: \", x.shape)\n",
    "print(\"Contiguous stride: \", x.stride())\n",
    "print()\n",
    "\n",
    "xcl = x.to(memory_format=torch.channels_last)\n",
    "print(\"Channels-Last shape: \", xcl.shape)\n",
    "print(\"Channels-Last stride: \", xcl.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MatMul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://triton-lang.org/main/_images/grouped_vs_row_major_ordering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# `triton.jit`'ed functions can be auto-tuned by using the `triton.autotune` decorator, which consumes:\n",
    "#   - A list of `triton.Config` objects that define different configurations of\n",
    "#       meta-parameters (e.g., `BLOCK_SIZE_M`) and compilation options (e.g., `num_warps`) to try\n",
    "#   - An auto-tuning *key* whose change in values will trigger evaluation of all the\n",
    "#       provided configs\n",
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3,\n",
    "                      num_warps=8),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4,\n",
    "                      num_warps=4),\n",
    "        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5,\n",
    "                      num_warps=2),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "        # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "        # by to get the element one row down (A has M rows).\n",
    "        stride_am, stride_ak,  #\n",
    "        stride_bk, stride_bn,  #\n",
    "        stride_cm, stride_cn,\n",
    "        # Meta-parameters\n",
    "        BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #\n",
    "        GROUP_SIZE_M: tl.constexpr,  #\n",
    "        ACTIVATION: tl.constexpr  #\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    \n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + (pid % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetic` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator += tl.dot(a, b)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * stride_bk\n",
    "    # You can fuse arbitrary activation functions here\n",
    "    # while the accumulator is still in FP32!\n",
    "    \n",
    "    if ACTIVATION == \"leaky_relu\":\n",
    "        accumulator = leaky_relu(accumulator)\n",
    "    c = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    tl.store(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "# We can fuse `leaky_relu` by providing it as an `ACTIVATION` meta-parameter in `_matmul`.\n",
    "@triton.jit\n",
    "def leaky_relu(x):\n",
    "    x = x + 1\n",
    "    return tl.where(x >= 0, x, 0.01 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a, b, activation=\"\", verbose=False):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    assert b.is_contiguous(), \"Matrix B must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"{M=} {K=} {N=}\")\n",
    "        print(f\"{a.stride(0)=}, {a.stride(1)=}\")\n",
    "        print(f\"{b.stride(0)=}, {b.stride(1)=}\")\n",
    "        print(f\"{c.stride(0)=}, {c.stride(1)=}\")\n",
    "\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "        ACTIVATION=activation  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "triton_output = matmul(a, b, activation=\"leaky_relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yet another profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 33)],  # Different possible values for `x_name`\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=['cublas', 'triton'],\n",
    "        # Label name for the lines\n",
    "        line_names=[\"cuBLAS\", \"Triton\"],\n",
    "        # Line styles\n",
    "        styles=[('green', '-'), ('blue', '-')],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    ))\n",
    "def benchmark(M, N, K, provider):\n",
    "    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn((K, N), device='cuda', dtype=torch.float16)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'cublas':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Attention\n",
    "\n",
    "Flash Attention is a highly efficient attention mechanism designed to significantly reduce the computational cost and memory usage associated with processing large-scale inputs in Transformer models. It leverages a novel algorithm that allows for the computation of attention weights and the aggregation of context vectors in a single pass, improving both speed and efficiency. This innovation enables the handling of longer sequences in natural language processing tasks and other applications, making Transformers more scalable and practical for a broader range of datasets and computational constraints.\n",
    "\n",
    "[FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n",
    "\n",
    "[FlassAttention2](https://arxiv.org/abs/2307.08691)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyAttention(nn.Module):\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        return F.scaled_dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_attention = DummyAttention()\n",
    "\n",
    "dtype = torch.float16\n",
    "# qkv = torch.randn(64, 1024, 3, 8, 128, dtype=dtype, device=\"cuda\") \n",
    "# kv = torch.randn(64, 1024, 2, 8, 128, dtype=dtype, device=\"cuda\")\n",
    "q = torch.randn(64, 1024, 8, 128, dtype=dtype, device=\"cuda\")\n",
    "k = torch.randn(64, 1024, 8, 128, dtype=dtype, device=\"cuda\")\n",
    "v = torch.randn(64, 1024, 8, 128, dtype=dtype, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.backends.cuda import sdp_kernel, SDPBackend\n",
    "\n",
    "backend_map = {\n",
    "    SDPBackend.MATH: {\"enable_math\": True, \"enable_flash\": False, \"enable_mem_efficient\": False},\n",
    "    SDPBackend.FLASH_ATTENTION: {\"enable_math\": False, \"enable_flash\": True, \"enable_mem_efficient\": False},\n",
    "    SDPBackend.EFFICIENT_ATTENTION: {\"enable_math\": False, \"enable_flash\": False, \"enable_mem_efficient\": True}\n",
    "}\n",
    "\n",
    "with sdp_kernel(**backend_map[SDPBackend.FLASH_ATTENTION]):\n",
    "    p = profile_model(dummy_attention, {'q': q, 'k': k, 'v': v}, ProfilerConfig(amp=False))\n",
    "    print(p.key_averages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sdp_kernel(**backend_map[SDPBackend.EFFICIENT_ATTENTION]):\n",
    "    p = profile_model(dummy_attention, {'q': q, 'k': k, 'v': v}, ProfilerConfig(amp=False))\n",
    "    print(p.key_averages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
