{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/justheuristic/4c82ef4d448ce62cb5459484f66f56aa/practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H91Iz3PiAFEK"
      },
      "source": [
        "### Practice 1: Parallel GloVe\n",
        "\n",
        "In this assignment we'll build parallel GloVe training from scratch. Well, almost from scratch:\n",
        "* we'll use python's builtin [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html) library\n",
        "* and learn to access numpy arrays from multiple processes!\n",
        "\n",
        "![img](https://i.imgur.com/YHluIBo.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9iWJGzIAFEO"
      },
      "outputs": [],
      "source": [
        "%env MKL_NUM_THREADS=1\n",
        "%env NUMEXPR_NUM_THREADS=1\n",
        "%env OMP_NUM_THREADS=1\n",
        "# set numpy to single-threaded mode for benchmarking\n",
        "\n",
        "!pip install --upgrade nltk datasets tqdm\n",
        "!wget https://raw.githubusercontent.com/mryab/efficient-dl-systems/main/week02_distributed/utils.py -O utils.py\n",
        "\n",
        "import time, random\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVojuqhGAFES"
      },
      "source": [
        "### Multiprocessing basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr6HV2PjAFEU"
      },
      "outputs": [],
      "source": [
        "def foo(i):\n",
        "    \"\"\" Imagine particularly computation-heavy function... \"\"\"\n",
        "    print(end=f\"Began foo({i})...\\n\")\n",
        "    result = np.sin(i)\n",
        "    time.sleep(abs(result))\n",
        "    print(end=f\"Finished foo({i}) = {result:.3f}.\\n\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOOPAmDdAFEW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "results_naive = [foo(i) for i in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzBc-wA2AFEX"
      },
      "source": [
        "Same, but with multiple processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQPXLqdlAFEY"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "processes = []\n",
        "for i in range(10):\n",
        "    proc = mp.Process(target=foo, args=[i])\n",
        "    processes.append(proc)\n",
        "\n",
        "print(f\"Created {len(processes)} processes!\")\n",
        "\n",
        "# start in parallel\n",
        "for proc in processes:\n",
        "    proc.start()\n",
        "    \n",
        "# wait for everyone finish\n",
        "for proc in processes:\n",
        "    proc.join()  # wait until proc terminates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-e91ez4AFEZ"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "Great! But how do we collect the values?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJCsKVJsAFEb"
      },
      "source": [
        "__Solution 1:__ with pipes!\n",
        "\n",
        "Two \"sides\", __one__ process from each side\n",
        "* `pipe_side.send(data)` - throw data into the pipe (do not wait for it to be read)\n",
        "* `data = pipe_side.recv()` - read data. If there is none, wait for someone to send data\n",
        "\n",
        "__Rules:__\n",
        "* each side should be controlled by __one__ process\n",
        "* data transferred through pipes must be serializable\n",
        "* if `duplex=True`, processes can communicate both ways\n",
        "* if `duplex=False`, \"left\" receives and \"right\" side sends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRI8OvzwAFEd"
      },
      "outputs": [],
      "source": [
        "side_A, side_B = mp.Pipe()\n",
        "\n",
        "side_A.send(123)\n",
        "side_A.send({'ololo': np.random.randn(3)})\n",
        "\n",
        "print(\"side_B.recv() -> \", side_B.recv())\n",
        "print(\"side_B.recv() -> \", side_B.recv())\n",
        "\n",
        "# note: calling recv a third will hang the process (waiting for someone to send data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCKp1tq1AFEe"
      },
      "outputs": [],
      "source": [
        "def compute_and_send(i, output_pipe):\n",
        "    print(end=f\"Began compute_and_send({i})...\\n\")\n",
        "    result = np.sin(i)\n",
        "    time.sleep(abs(result))\n",
        "    print(end=f\"Finished compute_and_send({i}) = {result:.3f}.\\n\")\n",
        "    \n",
        "    output_pipe.send(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j1mKkn5AFEe"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "result_pipes = []\n",
        "\n",
        "for i in range(10):\n",
        "    \n",
        "    side_A, side_B = mp.Pipe(duplex=False)\n",
        "    # note: duplex=False means that side_B can only send\n",
        "    # and side_A can only recv. Otherwise its bidirectional\n",
        "    result_pipes.append(side_A)\n",
        "    proc = mp.Process(target=compute_and_send, args=[i, side_B])\n",
        "    proc.start()\n",
        "\n",
        "print(\"MAIN PROCESS: awaiting results...\")\n",
        "for pipe in result_pipes:\n",
        "    print(f\"MAIN_PROCESS: received {pipe.recv()}\")\n",
        "print(\"MAIN PROCESS: done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woOzj-h9AFEf"
      },
      "source": [
        "__Solution 2:__ with multiprocessing templates\n",
        "\n",
        "Multiprocessing contains some template data structures that help you communicate between processes.\n",
        "\n",
        "One such structure is `mp.Queue` a Queue that can be accessed by multiple processes in parallel.\n",
        "* `queue.put` adds the value to the queue, accessible by all other processes\n",
        "* `queue.get` returns the earliest added value and removes it from queue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8mPT8uvAFEg"
      },
      "outputs": [],
      "source": [
        "queue = mp.Queue()\n",
        "\n",
        "def func_A(queue):\n",
        "    print(\"A: awaiting queue...\")\n",
        "    print(\"A: retreived from queue:\", queue.get())\n",
        "    print(\"A: awaiting queue...\")\n",
        "    print(\"A: retreived from queue:\", queue.get())\n",
        "    print(\"A: done!\")\n",
        "\n",
        "def func_B(i, queue):\n",
        "    value = np.random.rand()\n",
        "    time.sleep(value)\n",
        "    print(f\"proc_B{i}: putting more stuff into queue!\")\n",
        "    queue.put(value)\n",
        "    \n",
        "\n",
        "proc_A = mp.Process(target=func_A, args=[queue])\n",
        "proc_A.start();\n",
        "\n",
        "proc_B1 = mp.Process(target=func_B, args=[1, queue])\n",
        "proc_B2 = mp.Process(target=func_B, args=[2, queue])\n",
        "proc_B1.start(), proc_B2.start();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-rzgIRPAFEh"
      },
      "source": [
        "__Important note:__ you can see that the two values above are identical.\n",
        "\n",
        "This is because proc_B1 and proc_B2 were forked (cloned) with __the same random state!__\n",
        "\n",
        "To mitigate this issue, run `np.random.seed()` in each process (same for torch, tensorflow).\n",
        "\n",
        "<details>\n",
        "    <summary>In fact, please go and to that <b>right now!</b></summary>\n",
        "    <img src='https://media.tenor.com/images/32c950f36a61ec7e5060f5eee9140396/tenor.gif' height=200px>\n",
        "</details>\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "__Less important note:__ `mp.Queue vs mp.Pipe`\n",
        "- pipes are much faster for 1v1 communication\n",
        "- queues support arbitrary number of processes\n",
        "- queues are implemented with pipes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHck3-qUAFEi"
      },
      "source": [
        "### GloVe preprocessing\n",
        "\n",
        "Before we can train GloVe, we must first construct the co-occurence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNC7L4avAFEj"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "data = datasets.load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
        "# for fast debugging, you can temporarily use smaller data: 'wikitext-2-raw-v1'\n",
        "\n",
        "print(\"Example:\", data['train']['text'][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctIVLlsaAFEk"
      },
      "source": [
        "__First,__ let's build a vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3LTURX-AFEl"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from nltk.tokenize import NLTKWordTokenizer\n",
        "tokenizer = NLTKWordTokenizer()\n",
        "\n",
        "def count_tokens(lines, top_k=None):\n",
        "    \"\"\" Tokenize lines and return top_k most frequent tokens and their counts \"\"\"\n",
        "    sent_tokens = tokenizer.tokenize_sents(map(str.lower, lines))\n",
        "    token_counts = Counter([token for sent in sent_tokens for token in sent])\n",
        "    return Counter(dict(token_counts.most_common(top_k)))\n",
        "\n",
        "count_tokens(data['train']['text'][:100], top_k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCyX3nruAFEm"
      },
      "outputs": [],
      "source": [
        "# sequential algorithm\n",
        "texts = data['train']['text'][:100_000]\n",
        "vocabulary_size = 32_000\n",
        "batch_size = 10_000\n",
        "\n",
        "token_counts = Counter()\n",
        "\n",
        "for batch_start in trange(0, len(texts), batch_size):\n",
        "    batch_texts = texts[batch_start: batch_start + batch_size]\n",
        "    batch_counts = count_tokens(batch_texts, top_k=vocabulary_size)\n",
        "    token_counts += Counter(batch_counts)\n",
        "\n",
        "# save for later\n",
        "token_counts_reference = Counter(token_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nsYnwdFAFEo"
      },
      "source": [
        "### Let's parallelize (20% points)\n",
        "\n",
        "__Your task__ is to speed up the code above using using multiprocessing with queues and/or pipes _(or [shared memory](https://docs.python.org/3/library/multiprocessing.shared_memory.html) if you're up to that)_.\n",
        "\n",
        "__Kudos__ for implementing some form of global progress tracker (like progressbar above)\n",
        "\n",
        "Please do **not** use task executors (e.g. mp.pool, joblib, ProcessPoolExecutor), we'll get to them soon!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4IporOCAFEo"
      },
      "outputs": [],
      "source": [
        "texts = data['train']['text'][:100_000]\n",
        "vocabulary_size = 32_000\n",
        "batch_size = 10_000\n",
        "\n",
        "<YOUR CODE HERE>\n",
        "\n",
        "token_counts = <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygCy9JSKAFEo"
      },
      "outputs": [],
      "source": [
        "assert len(token_counts) == len(token_counts_reference)\n",
        "for token, ref_count in token_counts_reference.items():\n",
        "    assert token in token_counts, token\n",
        "    assert token_counts[token] == ref_count, token\n",
        "\n",
        "token_counts = Counter(dict(token_counts.most_common(vocabulary_size)))\n",
        "\n",
        "vocabulary = sorted(token_counts.keys())\n",
        "token_to_index = {token: i for i, token in enumerate(vocabulary)}\n",
        "assert len(vocabulary) == vocabulary_size, len(vocabulary)\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgALuOgdAFEp"
      },
      "source": [
        "### Part 2: Construct co-occurence matrix (10% points)\n",
        "\n",
        "\n",
        "__Your task__ is to count co-occurences of all words in a 5-token window. Please use the same preprocessing and tokenizer as above.\n",
        "\n",
        "__Also:__ please only count words that are in the vocabulary defined above.\n",
        "\n",
        "![image.png](https://i.imgur.com/2XmhYn5.png)\n",
        "\n",
        "\n",
        "\n",
        "__Note:__ this task and everything below has no instructions/interfaces. We will design those interfaces __together on the seminar.__\n",
        "\n",
        "The detailed instructions will appear later this night after the seminar is over.\n",
        "However, if you want to write the code from scratch, feel free to ignore these instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N1acrkUAFEq"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "def count_token_cooccurences(lines, vocabulary_size: int, window_size: int):\n",
        "    \"\"\" Tokenize lines and return top_k most frequent tokens and their counts \"\"\"\n",
        "    cooc = Counter()\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = tokenizer.tokenize(line.lower())\n",
        "        token_ix = [token_to_index[token] for token in tokens\n",
        "                    if token in token_to_index]\n",
        "        \n",
        "        for i in range(len(token_ix)):\n",
        "            for j in range(max(i - window_size, 0),\n",
        "                           min(i + window_size + 1, len(token_ix))):\n",
        "                if i != j:\n",
        "                    cooc[token_ix[i], token_ix[j]] += 1 / abs(i - j)\n",
        "    return counter_to_matrix(cooc, vocabulary_size)\n",
        "\n",
        "def counter_to_matrix(counter, vocabulary_size):\n",
        "    keys, values = zip(*counter.items())\n",
        "    ii, jj = zip(*keys)\n",
        "    return scipy.sparse.csr_matrix((values, (ii, jj)), dtype='float32',\n",
        "                                   shape=(vocabulary_size, vocabulary_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpvoJI4TAFEq"
      },
      "outputs": [],
      "source": [
        "texts = data['train']['text'][:100_000]\n",
        "batch_size = 10_000\n",
        "window_size = 5\n",
        "\n",
        "cooc = scipy.sparse.csr_matrix((vocabulary_size, vocabulary_size), dtype='float32')\n",
        "for batch_start in trange(0, len(texts), batch_size):\n",
        "    batch_texts = texts[batch_start: batch_start + batch_size]\n",
        "    batch_cooc = count_token_cooccurences(batch_texts, vocabulary_size, window_size)\n",
        "    cooc += batch_cooc\n",
        "    \n",
        "\n",
        "# This cell will run for a couple minutes, go get some tea!\n",
        "reference_cooc = cooc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-iwaVloAFEr"
      },
      "source": [
        "__Simple parallelism with `mp.Pool`__\n",
        "\n",
        "Many standard parallel tasks, such as applying the same function to an array of inputs, can be automated by using prebuilt primitives such as Pool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCe_-cMzAFEr"
      },
      "outputs": [],
      "source": [
        "def foo(i):\n",
        "    print(f'Began foo({i})', flush=True)\n",
        "    time.sleep(1)\n",
        "    print(f'Done foo({i})', flush=True)\n",
        "    return i ** 2\n",
        "\n",
        "with mp.Pool(processes=8) as pool:\n",
        "    results = pool.map(foo, range(5))\n",
        "    \n",
        "# or use iterators:\n",
        "# for result in pool.imap(foo, range(5)):\n",
        "#    print('Got', result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-45tfQN7AFEs"
      },
      "source": [
        "__Our next step__ is to implement a parallel version of co-occurence computation using the process pool functionality.\n",
        "\n",
        "There are multiple alternatives to mp.Pool: [joblib.Parallel](https://joblib.readthedocs.io/en/latest/), [ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor), [ipyparallel](https://github.com/ipython/ipyparallel), etc. Feel free to use whichever one you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSp_OQzxAFEs",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "texts = data['train']['text'][:100_000]\n",
        "batch_size = 10_000\n",
        "window_size = 5\n",
        "\n",
        "<YOUR CODE HERE>\n",
        "\n",
        "cooc = <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPG1qkXIAFEs"
      },
      "outputs": [],
      "source": [
        "assert isinstance(cooc, scipy.sparse.csr_matrix)\n",
        "assert cooc.nnz == reference_cooc.nnz\n",
        "for _ in trange(100_000):\n",
        "    i, j = np.random.randint(0, vocabulary_size, size=2)\n",
        "    assert np.allclose(cooc[i, j], reference_cooc[i, j])\n",
        "\n",
        "print(\"Perfect!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zanNUlMpAFEt"
      },
      "source": [
        "__Preprocess and save the full data__\n",
        "\n",
        "Finally, let's run the preprocessing code for the entire dataset and save the results for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQLkgNtUAFEt"
      },
      "outputs": [],
      "source": [
        "texts = data['train']['text']\n",
        "vocabulary_size = 32_000\n",
        "batch_size = 10_000\n",
        "window_size = 5\n",
        "\n",
        "# YOUR CODE: compute both vocabulary and cooc on the entire training corpora and save the results\n",
        "\n",
        "<A WHOLE LOT OF YOUR CODE>\n",
        "\n",
        "token_counts = <...>\n",
        "cooc = <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdxoorbBAFEu"
      },
      "outputs": [],
      "source": [
        "assert len(vocabulary) == vocabulary_size\n",
        "assert cooc.shape == (vocabulary_size, vocabulary_size)\n",
        "assert 440_000_000 < np.sum(cooc) < 450_000_000\n",
        "assert 0.05 < cooc.nnz / vocabulary_size ** 2 < 0.06"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZXhjIFJAFEu"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('preprocessed_data.pcl', 'wb') as f:\n",
        "    pickle.dump((vocabulary, cooc.tocoo()), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rN314cCAFEu"
      },
      "source": [
        "### Finally, GloVe!  (20% points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNLaUatnAFEu"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('preprocessed_data.pcl', 'rb') as f:\n",
        "    vocabulary, cooc = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e401y6JAFEu"
      },
      "source": [
        "### Weight function\n",
        "![image.png](https://i.imgur.com/Cdu6BJ5.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O7lIzauAFEw"
      },
      "outputs": [],
      "source": [
        "def compute_loss_weights(counts_ij):\n",
        "    \"\"\" Compute GloVe weights \"\"\"\n",
        "    <YOUR CODE HERE>\n",
        "    return <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTya47l8AFEw"
      },
      "outputs": [],
      "source": [
        "dummy_weights = compute_loss_weights(np.arange(0, 200, 30))\n",
        "dummy_reference_weights = [0. , 0.40536, 0.681731, 0.92402, 1. , 1. , 1.]\n",
        "assert np.allclose(dummy_weights, dummy_reference_weights, rtol=1e-4, atol=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4a6DJqjAFEw"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "![img](https://i.imgur.com/bkEBBLk.png)\n",
        "\n",
        "\n",
        "__The goal__ is to compute the loss function as per formula above. The only difference is that you should take _mean_ over batch instead of sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bempH2-4AFEw"
      },
      "outputs": [],
      "source": [
        "def compute_loss(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij):\n",
        "    \"\"\"\n",
        "    Compute GloVe loss function given embeddings, biases and targets\n",
        "    \n",
        "    :param emb_ii, emb_jj: vectors of left- and right-side words, shape: [batch_size, embedding_dimension]\n",
        "    :param bias_ii, bias_jj: biases for left- and right-side words, shape: [batch_size]\n",
        "    :param counts_ij: values from co-occurence matrix, shape: [batch_size]\n",
        "    :returns: mean GloVe loss over batch, shape: scalar\n",
        "    \"\"\"\n",
        "    weights = compute_loss_weights(counts_ij)\n",
        "    target = np.log(counts_ij)\n",
        "    \n",
        "    <YOUR CODE>\n",
        "    \n",
        "    return <...>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFO8TyMkAFEx"
      },
      "outputs": [],
      "source": [
        "dummy_emb_ii = np.sin(np.linspace(0, 10, 40)).reshape(4, 10)\n",
        "dummy_emb_jj = np.cos(np.linspace(10, 20, 40)).reshape(4, 10)\n",
        "dummy_bias_ii = np.linspace(-3, 2, 4)\n",
        "dummy_bias_jj = np.linspace(4, -1, 4)\n",
        "dummy_counts_ij = np.abs(np.sin(np.linspace(1, 100, 4)) * 150)\n",
        "\n",
        "dummy_loss = compute_loss(dummy_emb_ii, dummy_emb_jj, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij)\n",
        "\n",
        "assert np.shape(dummy_loss) == ()\n",
        "assert np.allclose(dummy_loss, 1.84289356)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c81syAvPAFEx"
      },
      "outputs": [],
      "source": [
        "def compute_grads(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij):\n",
        "    \"\"\"\n",
        "    Compute gradients of GloVe loss with respect to emb_ii/jj and bias_ii/jj\n",
        "    Assume the same parameter shapes as above\n",
        "    :returns: (grad_wrt_emb_ii, grad_wrt_emb_jj, grad_wrt_bias_ii, grad_wrt_bias_jj)\n",
        "    \"\"\"\n",
        "    <YOUR CODE>\n",
        "    \n",
        "    return <...>, <...>, <...>, <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMKNKChwAFEx"
      },
      "outputs": [],
      "source": [
        "grad_emb_ii, grad_emb_jj, grad_bias_ii, grad_bias_jj = compute_grads(\n",
        "    dummy_emb_ii, dummy_emb_jj, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij)\n",
        "\n",
        "assert np.shape(grad_emb_ii) == np.shape(grad_emb_jj) == np.shape(dummy_emb_ii)\n",
        "assert np.shape(grad_bias_ii) == np.shape(grad_bias_jj) == np.shape(dummy_bias_ii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBrXg99RAFEy"
      },
      "outputs": [],
      "source": [
        "from utils import eval_numerical_gradient\n",
        "reference_grad_bias_ii = eval_numerical_gradient(\n",
        "    lambda x: compute_loss(dummy_emb_ii, dummy_emb_jj, x, dummy_bias_jj, dummy_counts_ij),\n",
        "    x=dummy_bias_ii)\n",
        "\n",
        "assert np.allclose(reference_grad_bias_ii, grad_bias_ii, rtol=1e-4, atol=1e-3)\n",
        "print(\"dL/db[ii] OK\")\n",
        "\n",
        "reference_grad_bias_jj = eval_numerical_gradient(\n",
        "    lambda x: compute_loss(dummy_emb_ii, dummy_emb_jj, dummy_bias_ii, x, dummy_counts_ij),\n",
        "    x=dummy_bias_jj)\n",
        "\n",
        "assert np.allclose(reference_grad_bias_jj, grad_bias_jj, rtol=1e-4, atol=1e-3)\n",
        "print(\"dL/db[jj] OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Unp2sHAFEy"
      },
      "outputs": [],
      "source": [
        "reference_grad_emb_ii = eval_numerical_gradient(\n",
        "    lambda x: compute_loss(x, dummy_emb_jj, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij),\n",
        "    x=dummy_emb_ii)\n",
        "\n",
        "assert np.allclose(reference_grad_emb_ii, grad_emb_ii, rtol=1e-4, atol=1e-3)\n",
        "print(\"dL/dEmb[ii] OK\")\n",
        "\n",
        "\n",
        "reference_grad_emb_jj = eval_numerical_gradient(\n",
        "    lambda x: compute_loss(dummy_emb_ii, x, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij),\n",
        "    x=dummy_emb_jj)\n",
        "\n",
        "assert np.allclose(reference_grad_emb_jj, grad_emb_jj, rtol=1e-4, atol=1e-3)\n",
        "print(\"dL/dEmb[ii] OK\")\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiRd2ERLAFEz"
      },
      "source": [
        "### Part 3: Parallel GloVe training (50% points)\n",
        "\n",
        "Finally, let's write the actual parameter server for parallel GloVe training. In order to do so efficiently, we shall use shared memory instead of pipes.\n",
        "\n",
        "You can find an example of how shared memory works below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8AF358WAFE0"
      },
      "source": [
        "### Demo: shared memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X0UxoWpAFE0"
      },
      "outputs": [],
      "source": [
        "def make_shared_array(shape, dtype, fill=None, lock=True):\n",
        "    \"\"\" Create a numpy array that is shared across processes. \"\"\"\n",
        "    size = int(np.prod(shape))\n",
        "    ctype = np.ctypeslib.as_ctypes_type(dtype)\n",
        "    if lock:\n",
        "        x_mp = mp.Array(ctype, size, lock=True).get_obj()\n",
        "    else:\n",
        "        x_mp = mp.Array(ctype, size, lock=False)\n",
        "    array = np.ctypeslib.as_array(x_mp)\n",
        "    if fill is not None:\n",
        "        array[...] = fill\n",
        "    return np.reshape(array, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lc775zDAFE1"
      },
      "outputs": [],
      "source": [
        "shared_array = make_shared_array((5, 5), 'float32', fill=1)\n",
        "normal_array = np.ones((5, 5), 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXishO2FAFE2"
      },
      "outputs": [],
      "source": [
        "def proc_A():\n",
        "    time.sleep(0.5)\n",
        "    print(\"A: setting value at [2, 3]\")\n",
        "    shared_array[2, 3] = 42\n",
        "    normal_array[2, 3] = 42\n",
        "    time.sleep(1)\n",
        "    print(f\"A: value after 1.5s: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")\n",
        "    \n",
        "def proc_B():\n",
        "    print(f\"B: initial value: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")\n",
        "    time.sleep(1)\n",
        "    print(f\"B: value after 1s: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")\n",
        "    print(\"B: dividing value at [2, 3] by 2\")\n",
        "    shared_array[2, 3] /= 2\n",
        "    normal_array[2, 3] /= 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81N-7Xi9AFE3"
      },
      "outputs": [],
      "source": [
        "mp.Process(target=proc_A).start()\n",
        "mp.Process(target=proc_B).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWm-b7dRAFE4"
      },
      "outputs": [],
      "source": [
        "# the same can be done with individual values:\n",
        "x = mp.Value(np.ctypeslib.as_ctypes_type(np.int32))\n",
        "x.value += 1 # shared across all processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG-du4ddAFE6"
      },
      "source": [
        "__So, let's put all trainable parameters in shared memory!__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1JB__1kAFE6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class SharedEmbeddings:\n",
        "    \"\"\"\n",
        "    Word embeddings trainable parameters, allocated in shared memory\n",
        "    \"\"\"\n",
        "    def __init__(self, vocabulary_size: int, embedding_dimension: int, init_scale: float = 0.01):\n",
        "        self.embeddings = make_shared_array([vocabulary_size, embedding_dimension], np.float32, lock=False)\n",
        "        self.embeddings[...] = np.random.randn(*self.embeddings.shape) * init_scale\n",
        "        \n",
        "        self.biases = make_shared_array([vocabulary_size], np.float32, fill=0.0, lock=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr5PV0MfAFE6"
      },
      "source": [
        "### Training (single-core baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWJrOBAAAFE7"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "max_steps = 10 ** 6\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "timestep_history = []\n",
        "loss_history = []\n",
        "\n",
        "model = SharedEmbeddings(vocabulary_size, embedding_dimension=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYwgvX5CAFE7"
      },
      "outputs": [],
      "source": [
        "for t in trange(max_steps):\n",
        "    batch_ix = np.random.randint(0, len(cooc.row), size=batch_size)\n",
        "    ii, jj, counts_ij = cooc.row[batch_ix], cooc.col[batch_ix], cooc.data[batch_ix]\n",
        "    \n",
        "    # Compute gradients\n",
        "    emb_ii, emb_jj, bias_ii, bias_jj = \\\n",
        "      model.embeddings[ii], model.embeddings[jj], model.biases[ii], model.biases[jj]\n",
        "    \n",
        "    grad_emb_ii, grad_emb_jj, grad_bias_ii, grad_bias_jj = compute_grads(\n",
        "        emb_ii, emb_jj, bias_ii, bias_jj, counts_ij)\n",
        "    \n",
        "    # SGD step\n",
        "    model.embeddings[ii] -= learning_rate * grad_emb_ii\n",
        "    model.embeddings[jj] -= learning_rate * grad_emb_jj\n",
        "    model.biases[ii] -= learning_rate * grad_bias_ii\n",
        "    model.biases[jj] -= learning_rate * grad_bias_jj\n",
        "    \n",
        "    if t % 10_000 == 0:\n",
        "        batch_ix = np.random.randint(0, len(cooc.row), size=4096)\n",
        "        ii, jj, counts_ij = cooc.row[batch_ix], cooc.col[batch_ix], cooc.data[batch_ix]\n",
        "        emb_ii, emb_jj, bias_ii, bias_jj = \\\n",
        "            model.embeddings[ii], model.embeddings[jj], model.biases[ii], model.biases[jj]\n",
        "        \n",
        "        timestep_history.append(time.perf_counter() - start_time)\n",
        "        loss_history.append(compute_loss(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij))\n",
        "        clear_output(True)\n",
        "        plt.plot(timestep_history, loss_history)\n",
        "        plt.xlabel('training time(seconds)')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noZiGvffAFE8"
      },
      "source": [
        "__Now let's parallelize it!__\n",
        "\n",
        "The code above is cute, but it only uses one CPU core. Surely we can go faster!\n",
        "\n",
        "The main challenge in this week's seminar is to speed up GloVe training by all means necessary.\n",
        "\n",
        "Here's what you should do:\n",
        "* make multiple parallel workers, each training your model on different random data,\n",
        "* build some centralized means of progress tracking: track the average loss and the number of training steps,\n",
        "* implement workers in such a way that no process is left hanging after the training is over.\n",
        "\n",
        "\n",
        "Finally, please compare the loss / training time plot of your algorithm against the baseline.\n",
        "\n",
        "_Notes:_\n",
        "* Remember to set a different np.random.seed in each worker!\n",
        "* You can track the training progress either via mp.Pipe or via shared variables\n",
        "* It is better to separate training and plotting into different processes\n",
        "* If you want to prevent concurrent updates to shared memory, you can use [mp.Lock](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Lock) or similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTD4WQtjAFE9"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "max_steps = 10 ** 6\n",
        "start_time = time.perf_counter()\n",
        "timestep_history = []\n",
        "loss_history = []\n",
        "\n",
        "model = SharedEmbeddings(vocabulary_size, embedding_dimension=256)\n",
        "\n",
        "\n",
        "# <YOUR CODE HERE> - optional preparations, auxiliary functions, locks, pipes, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtS6YHzmAFE9"
      },
      "outputs": [],
      "source": [
        "# <YOUR CODE HERE> - actually train the model, track performance, and clean up at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME0lZHuKAFE-"
      },
      "source": [
        "Hello, I'm `______` and here's what i've done:\n",
        "\n",
        "* something"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "practice.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py38",
      "language": "python",
      "name": "py38"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
