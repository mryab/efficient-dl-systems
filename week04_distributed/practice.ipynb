{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H91Iz3PiAFEK"
      },
      "source": [
        "### Practice: Parallel GloVe\n",
        "\n",
        "In this assignment we'll build parallel GloVe training from scratch. Well, almost from scratch:\n",
        "* we'll use python's builtin [`multiprocessing`](https://docs.python.org/3/library/multiprocessing.html) library\n",
        "* and learn to access numpy arrays from multiple processes!\n",
        "\n",
        "![img](https://i.imgur.com/YHluIBo.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9iWJGzIAFEO"
      },
      "outputs": [],
      "source": [
        "%env MKL_NUM_THREADS=1\n",
        "%env NUMEXPR_NUM_THREADS=1\n",
        "%env OMP_NUM_THREADS=1\n",
        "# set numpy to single-threaded mode for benchmarking\n",
        "\n",
        "!pip install --upgrade nltk datasets tqdm   #                        v-- not a mistake\n",
        "!wget https://raw.githubusercontent.com/mryab/efficient-dl-systems/2023/week04_distributed/utils.py -O utils.py\n",
        "\n",
        "import time, random\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVojuqhGAFES"
      },
      "source": [
        "### Multiprocessing basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr6HV2PjAFEU"
      },
      "outputs": [],
      "source": [
        "def foo(i):\n",
        "    \"\"\" Imagine particularly computation-heavy function... \"\"\"\n",
        "    print(end=f\"Began foo({i})...\\n\")\n",
        "    result = np.sin(i)\n",
        "    time.sleep(abs(result))\n",
        "    print(end=f\"Finished foo({i}) = {result:.3f}.\\n\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOOPAmDdAFEW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "results_naive = [foo(i) for i in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzBc-wA2AFEX"
      },
      "source": [
        "Same, but with multiple processes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQPXLqdlAFEY"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "processes = []\n",
        "for i in range(10):\n",
        "    proc = mp.Process(target=foo, args=[i])\n",
        "    processes.append(proc)\n",
        "\n",
        "print(f\"Created {len(processes)} processes!\")\n",
        "\n",
        "# start in parallel\n",
        "for proc in processes:\n",
        "    proc.start()\n",
        "\n",
        "# wait for everyone finish\n",
        "for proc in processes:\n",
        "    proc.join()  # wait until proc terminates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-e91ez4AFEZ"
      },
      "source": [
        "Great! But how do we collect the values?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJCsKVJsAFEb"
      },
      "source": [
        "__Solution 1:__ with pipes!\n",
        "\n",
        "Two \"sides\", __one__ process from each side\n",
        "* `pipe_side.send(data)` - throw data into the pipe (do not wait for it to be read)\n",
        "* `data = pipe_side.recv()` - read data. If there is none, wait for someone to send data\n",
        "\n",
        "__Rules:__\n",
        "* each side should be controlled by __one__ process\n",
        "* data transferred through pipes must be serializable\n",
        "* if `duplex=True`, processes can communicate both ways\n",
        "* if `duplex=False`, \"left\" receives and \"right\" side sends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRI8OvzwAFEd"
      },
      "outputs": [],
      "source": [
        "side_A, side_B = mp.Pipe()\n",
        "\n",
        "side_A.send(123)\n",
        "side_A.send({'ololo': np.random.randn(3)})\n",
        "\n",
        "print(\"side_B.recv() -> \", side_B.recv())\n",
        "print(\"side_B.recv() -> \", side_B.recv())\n",
        "\n",
        "# note: calling recv a third will hang the process (waiting for someone to send data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCKp1tq1AFEe"
      },
      "outputs": [],
      "source": [
        "def compute_and_send(i, output_pipe):\n",
        "    print(end=f\"Began compute_and_send({i})...\\n\")\n",
        "    result = np.sin(i)\n",
        "    time.sleep(abs(result))\n",
        "    print(end=f\"Finished compute_and_send({i}) = {result:.3f}.\\n\")\n",
        "\n",
        "    output_pipe.send(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j1mKkn5AFEe"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "result_pipes = []\n",
        "\n",
        "for i in range(10):\n",
        "\n",
        "    side_A, side_B = mp.Pipe(duplex=False)\n",
        "    # note: duplex=False means that side_B can only send\n",
        "    # and side_A can only recv. Otherwise its bidirectional\n",
        "    result_pipes.append(side_A)\n",
        "    proc = mp.Process(target=compute_and_send, args=[i, side_B])\n",
        "    proc.start()\n",
        "\n",
        "print(\"MAIN PROCESS: awaiting results...\")\n",
        "for pipe in result_pipes:\n",
        "    print(f\"MAIN_PROCESS: received {pipe.recv()}\")\n",
        "print(\"MAIN PROCESS: done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woOzj-h9AFEf"
      },
      "source": [
        "__Solution 2:__ with multiprocessing templates\n",
        "\n",
        "Multiprocessing contains some template data structures that help you communicate between processes.\n",
        "\n",
        "One such structure is `mp.Queue` a Queue that can be accessed by multiple processes in parallel.\n",
        "* `queue.put` adds the value to the queue, accessible by all other processes\n",
        "* `queue.get` returns the earliest added value and removes it from queue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8mPT8uvAFEg"
      },
      "outputs": [],
      "source": [
        "queue = mp.Queue()\n",
        "\n",
        "def func_A(queue):\n",
        "    print(\"A: awaiting queue...\")\n",
        "    print(\"A: retreived from queue:\", queue.get())\n",
        "    print(\"A: awaiting queue...\")\n",
        "    print(\"A: retreived from queue:\", queue.get())\n",
        "    print(\"A: done!\")\n",
        "\n",
        "def func_B(i, queue):\n",
        "    value = np.random.rand()\n",
        "    time.sleep(value)\n",
        "    print(f\"proc_B{i}: putting more stuff into queue!\")\n",
        "    queue.put(value)\n",
        "\n",
        "\n",
        "proc_A = mp.Process(target=func_A, args=[queue])\n",
        "proc_A.start();\n",
        "\n",
        "proc_B1 = mp.Process(target=func_B, args=[1, queue])\n",
        "proc_B2 = mp.Process(target=func_B, args=[2, queue])\n",
        "proc_B1.start(), proc_B2.start();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-rzgIRPAFEh"
      },
      "source": [
        "__Important note:__ you can see that the two values above are identical.\n",
        "\n",
        "This is because proc_B1 and proc_B2 were forked (cloned) with __the same random state!__\n",
        "\n",
        "To mitigate this issue, run `np.random.seed()` in each process (same for torch, tensorflow).\n",
        "\n",
        "<details>\n",
        "    <summary>In fact, please go and to that <b>right now!</b></summary>\n",
        "    <img src='https://media.tenor.com/images/32c950f36a61ec7e5060f5eee9140396/tenor.gif' height=200px>\n",
        "</details>\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "__Less important note:__ `mp.Queue vs mp.Pipe`\n",
        "- pipes are much faster for 1v1 communication\n",
        "- queues support arbitrary number of processes\n",
        "- queues are implemented with pipes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHck3-qUAFEi"
      },
      "source": [
        "### GloVe preprocessing\n",
        "\n",
        "Before we can train GloVe, we must first construct the co-occurence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNC7L4avAFEj"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "data = datasets.load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
        "# for fast debugging, you can temporarily use smaller data: 'wikitext-2-raw-v1'\n",
        "\n",
        "print(\"Example:\", data['train']['text'][5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctIVLlsaAFEk"
      },
      "source": [
        "__First,__ let's build a vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3LTURX-AFEl"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from nltk.tokenize import NLTKWordTokenizer\n",
        "tokenizer = NLTKWordTokenizer()\n",
        "\n",
        "def count_tokens(lines, top_k=None):\n",
        "    \"\"\" Tokenize lines and return top_k most frequent tokens and their counts \"\"\"\n",
        "    sent_tokens = tokenizer.tokenize_sents(map(str.lower, lines))\n",
        "    token_counts = Counter([token for sent in sent_tokens for token in sent])\n",
        "    return Counter(dict(token_counts.most_common(top_k)))\n",
        "\n",
        "count_tokens(data['train']['text'][:100], top_k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCyX3nruAFEm"
      },
      "outputs": [],
      "source": [
        "# sequential algorithm\n",
        "texts = data['train']['text'][:100_000]\n",
        "vocabulary_size = 32_000\n",
        "batch_size = 10_000\n",
        "\n",
        "token_counts = Counter()\n",
        "\n",
        "for batch_start in trange(0, len(texts), batch_size):\n",
        "    batch_texts = texts[batch_start: batch_start + batch_size]\n",
        "    batch_counts = count_tokens(batch_texts, top_k=vocabulary_size)\n",
        "    token_counts += Counter(batch_counts)\n",
        "\n",
        "# save for later\n",
        "token_counts_reference = Counter(token_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nsYnwdFAFEo"
      },
      "source": [
        "### Let's parallelize (2 points)\n",
        "\n",
        "__Your task__ is to speed up the code above using using multiprocessing with queues and/or pipes _(or [shared memory](https://docs.python.org/3/library/multiprocessing.shared_memory.html) if you're up to that)_.\n",
        "\n",
        "__Kudos__ for implementing some form of global progress tracker (like progressbar above)\n",
        "\n",
        "Please do **not** use task executors (e.g. mp.pool, joblib, ProcessPoolExecutor), we'll get to them soon!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4IporOCAFEo"
      },
      "outputs": [],
      "source": [
        "texts = data['train']['text'][:100_000]\n",
        "vocabulary_size = 32_000\n",
        "batch_size = 10_000\n",
        "\n",
        "<YOUR CODE HERE>\n",
        "\n",
        "token_counts = <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygCy9JSKAFEo"
      },
      "outputs": [],
      "source": [
        "assert len(token_counts) == len(token_counts_reference)\n",
        "for token, ref_count in token_counts_reference.items():\n",
        "    assert token in token_counts, token\n",
        "    assert token_counts[token] == ref_count, token\n",
        "\n",
        "token_counts = Counter(dict(token_counts.most_common(vocabulary_size)))\n",
        "\n",
        "vocabulary = sorted(token_counts.keys())\n",
        "token_to_index = {token: i for i, token in enumerate(vocabulary)}\n",
        "assert len(vocabulary) == vocabulary_size, len(vocabulary)\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgALuOgdAFEp"
      },
      "source": [
        "### Part 2: Construct co-occurence matrix (1 point)\n",
        "\n",
        "\n",
        "__Your task__ is to count co-occurences of all words in a 5-token window. Please use the same preprocessing and tokenizer as above.\n",
        "\n",
        "__Also:__ please only count words that are in the vocabulary defined above.\n",
        "\n",
        "![image.png](https://i.imgur.com/2XmhYn5.png)\n",
        "\n",
        "\n",
        "\n",
        "__Note:__ this task and everything below has no instructions/interfaces. We will design those interfaces __together on the seminar.__\n",
        "\n",
        "The detailed instructions will appear later this night after the seminar is over.\n",
        "However, if you want to write the code from scratch, feel free to ignore these instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1N1acrkUAFEq"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "\n",
        "def count_token_cooccurences(lines, vocabulary_size: int, window_size: int):\n",
        "    \"\"\" Tokenize lines and return top_k most frequent tokens and their counts \"\"\"\n",
        "    cooc = Counter()\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = tokenizer.tokenize(line.lower())\n",
        "        token_ix = [token_to_index[token] for token in tokens\n",
        "                    if token in token_to_index]\n",
        "\n",
        "        for i in range(len(token_ix)):\n",
        "            for j in range(max(i - window_size, 0),\n",
        "                           min(i + window_size + 1, len(token_ix))):\n",
        "                if i != j:\n",
        "                    cooc[token_ix[i], token_ix[j]] += 1 / abs(i - j)\n",
        "    return counter_to_matrix(cooc, vocabulary_size)\n",
        "\n",
        "def counter_to_matrix(counter, vocabulary_size):\n",
        "    keys, values = zip(*counter.items())\n",
        "    ii, jj = zip(*keys)\n",
        "    return scipy.sparse.csr_matrix((values, (ii, jj)), dtype='float32',\n",
        "                                   shape=(vocabulary_size, vocabulary_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpvoJI4TAFEq"
      },
      "outputs": [],
      "source": [
        "texts = data['train']['text'][:100_000]\n",
        "batch_size = 10_000\n",
        "window_size = 5\n",
        "\n",
        "cooc = scipy.sparse.csr_matrix((vocabulary_size, vocabulary_size), dtype='float32')\n",
        "for batch_start in trange(0, len(texts), batch_size):\n",
        "    batch_texts = texts[batch_start: batch_start + batch_size]\n",
        "    batch_cooc = count_token_cooccurences(batch_texts, vocabulary_size, window_size)\n",
        "    cooc += batch_cooc\n",
        "\n",
        "\n",
        "# This cell will run for a couple minutes, go get some tea!\n",
        "reference_cooc = cooc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-iwaVloAFEr"
      },
      "source": [
        "__Simple parallelism with `mp.Pool`__\n",
        "\n",
        "Many standard parallel tasks, such as applying the same function to an array of inputs, can be automated by using prebuilt primitives such as Pool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCe_-cMzAFEr"
      },
      "outputs": [],
      "source": [
        "def foo(i):\n",
        "    print(f'Began foo({i})', flush=True)\n",
        "    time.sleep(1)\n",
        "    print(f'Done foo({i})', flush=True)\n",
        "    return i ** 2\n",
        "\n",
        "with mp.Pool(processes=8) as pool:\n",
        "    results = pool.map(foo, range(5))\n",
        "\n",
        "# or use iterators:\n",
        "# for result in pool.imap(foo, range(5)):\n",
        "#    print('Got', result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-45tfQN7AFEs"
      },
      "source": [
        "__Our next step__ is to implement a parallel version of co-occurence computation using the process pool functionality.\n",
        "\n",
        "There are multiple alternatives to mp.Pool: [joblib.Parallel](https://joblib.readthedocs.io/en/latest/), [ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor), [ipyparallel](https://github.com/ipython/ipyparallel), etc. Feel free to use whichever one you prefer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSp_OQzxAFEs",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "texts = data['train']['text'][:100_000]\n",
        "batch_size = 10_000\n",
        "window_size = 5\n",
        "\n",
        "<YOUR CODE HERE>\n",
        "\n",
        "cooc = <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPG1qkXIAFEs"
      },
      "outputs": [],
      "source": [
        "assert isinstance(cooc, scipy.sparse.csr_matrix)\n",
        "assert cooc.nnz == reference_cooc.nnz\n",
        "for _ in trange(100_000):\n",
        "    i, j = np.random.randint(0, vocabulary_size, size=2)\n",
        "    assert np.allclose(cooc[i, j], reference_cooc[i, j])\n",
        "\n",
        "print(\"Perfect!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zanNUlMpAFEt"
      },
      "source": [
        "__Preprocess and save the full data__\n",
        "\n",
        "Finally, let's run the preprocessing code for the entire dataset and save the results for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQLkgNtUAFEt"
      },
      "outputs": [],
      "source": [
        "texts = data['train']['text']\n",
        "vocabulary_size = 32_000\n",
        "batch_size = 10_000\n",
        "window_size = 5\n",
        "\n",
        "# YOUR CODE: compute both vocabulary and cooc on the entire training corpora and save the results\n",
        "\n",
        "<A WHOLE LOT OF YOUR CODE>\n",
        "\n",
        "token_counts = <...>\n",
        "cooc = <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdxoorbBAFEu"
      },
      "outputs": [],
      "source": [
        "assert len(vocabulary) == vocabulary_size\n",
        "assert cooc.shape == (vocabulary_size, vocabulary_size)\n",
        "assert 440_000_000 < np.sum(cooc) < 450_000_000\n",
        "assert 0.05 < cooc.nnz / vocabulary_size ** 2 < 0.06"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZXhjIFJAFEu"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('preprocessed_data.pcl', 'wb') as f:\n",
        "    pickle.dump((vocabulary, cooc.tocoo()), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rN314cCAFEu"
      },
      "source": [
        "### Finally, GloVe!  (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNLaUatnAFEu"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('preprocessed_data.pcl', 'rb') as f:\n",
        "    vocabulary, cooc = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e401y6JAFEu"
      },
      "source": [
        "### Weight function\n",
        "![image.png](https://i.imgur.com/Cdu6BJ5.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O7lIzauAFEw"
      },
      "outputs": [],
      "source": [
        "def compute_loss_weights(counts_ij):\n",
        "    \"\"\" Compute GloVe weights \"\"\"\n",
        "    <YOUR CODE HERE>\n",
        "    return <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTya47l8AFEw"
      },
      "outputs": [],
      "source": [
        "dummy_weights = compute_loss_weights(np.arange(0, 200, 30))\n",
        "dummy_reference_weights = [0. , 0.40536, 0.681731, 0.92402, 1. , 1. , 1.]\n",
        "assert np.allclose(dummy_weights, dummy_reference_weights, rtol=1e-4, atol=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4a6DJqjAFEw"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "![img](https://i.imgur.com/bkEBBLk.png)\n",
        "\n",
        "\n",
        "__The goal__ is to compute the loss function as per formula above. The only difference is that you should take _mean_ over batch instead of sum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bempH2-4AFEw"
      },
      "outputs": [],
      "source": [
        "def compute_loss(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij):\n",
        "    \"\"\"\n",
        "    Compute GloVe loss function given embeddings, biases and targets\n",
        "\n",
        "    :param emb_ii, emb_jj: vectors of left- and right-side words, shape: [batch_size, embedding_dimension]\n",
        "    :param bias_ii, bias_jj: biases for left- and right-side words, shape: [batch_size]\n",
        "    :param counts_ij: values from co-occurence matrix, shape: [batch_size]\n",
        "    :returns: mean GloVe loss over batch, shape: scalar\n",
        "    \"\"\"\n",
        "    weights = compute_loss_weights(counts_ij)\n",
        "    target = np.log(counts_ij)\n",
        "\n",
        "    <YOUR CODE>\n",
        "\n",
        "    return <...>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFO8TyMkAFEx"
      },
      "outputs": [],
      "source": [
        "dummy_emb_ii = np.sin(np.linspace(0, 10, 40)).reshape(4, 10)\n",
        "dummy_emb_jj = np.cos(np.linspace(10, 20, 40)).reshape(4, 10)\n",
        "dummy_bias_ii = np.linspace(-3, 2, 4)\n",
        "dummy_bias_jj = np.linspace(4, -1, 4)\n",
        "dummy_counts_ij = np.abs(np.sin(np.linspace(1, 100, 4)) * 150)\n",
        "\n",
        "dummy_loss = compute_loss(dummy_emb_ii, dummy_emb_jj, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij)\n",
        "\n",
        "assert np.shape(dummy_loss) == ()\n",
        "assert np.allclose(dummy_loss, 1.84289356)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c81syAvPAFEx"
      },
      "outputs": [],
      "source": [
        "def compute_grads(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij):\n",
        "    \"\"\"\n",
        "    Compute gradients of GloVe loss with respect to emb_ii/jj and bias_ii/jj\n",
        "    Assume the same parameter shapes as above\n",
        "    :returns: (grad_wrt_emb_ii, grad_wrt_emb_jj, grad_wrt_bias_ii, grad_wrt_bias_jj)\n",
        "    \"\"\"\n",
        "    <YOUR CODE>\n",
        "\n",
        "    return <...>, <...>, <...>, <...>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMKNKChwAFEx"
      },
      "outputs": [],
      "source": [
        "grad_emb_ii, grad_emb_jj, grad_bias_ii, grad_bias_jj = compute_grads(\n",
        "    dummy_emb_ii, dummy_emb_jj, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij)\n",
        "\n",
        "assert np.shape(grad_emb_ii) == np.shape(grad_emb_jj) == np.shape(dummy_emb_ii)\n",
        "assert np.shape(grad_bias_ii) == np.shape(grad_bias_jj) == np.shape(dummy_bias_ii)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBrXg99RAFEy"
      },
      "outputs": [],
      "source": [
        "from utils import eval_numerical_gradient\n",
        "reference_grad_bias_ii = eval_numerical_gradient(\n",
        "    lambda x: compute_loss(dummy_emb_ii, dummy_emb_jj, x, dummy_bias_jj, dummy_counts_ij),\n",
        "    x=dummy_bias_ii)\n",
        "\n",
        "assert np.allclose(reference_grad_bias_ii, grad_bias_ii, rtol=1e-4, atol=1e-3)\n",
        "print(\"dL/db[ii] OK\")\n",
        "\n",
        "reference_grad_bias_jj = eval_numerical_gradient(\n",
        "    lambda x: compute_loss(dummy_emb_ii, dummy_emb_jj, dummy_bias_ii, x, dummy_counts_ij),\n",
        "    x=dummy_bias_jj)\n",
        "\n",
        "assert np.allclose(reference_grad_bias_jj, grad_bias_jj, rtol=1e-4, atol=1e-3)\n",
        "print(\"dL/db[jj] OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Unp2sHAFEy"
      },
      "outputs": [],
      "source": [
        "reference_grad_emb_ii = eval_numerical_gradient(\n",
        "    lambda x: compute_loss(x, dummy_emb_jj, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij),\n",
        "    x=dummy_emb_ii)\n",
        "\n",
        "assert np.allclose(reference_grad_emb_ii, grad_emb_ii, rtol=1e-4, atol=1e-3)\n",
        "print(\"dL/dEmb[ii] OK\")\n",
        "\n",
        "\n",
        "reference_grad_emb_jj = eval_numerical_gradient(\n",
        "    lambda x: compute_loss(dummy_emb_ii, x, dummy_bias_ii, dummy_bias_jj, dummy_counts_ij),\n",
        "    x=dummy_emb_jj)\n",
        "\n",
        "assert np.allclose(reference_grad_emb_jj, grad_emb_jj, rtol=1e-4, atol=1e-3)\n",
        "print(\"dL/dEmb[ii] OK\")\n",
        "\n",
        "print(\"All tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiRd2ERLAFEz"
      },
      "source": [
        "### Part 3: Parallel GloVe training (4 points)\n",
        "\n",
        "Finally, let's write the actual parameter server for parallel GloVe training. In order to do so efficiently, we shall use shared memory instead of pipes.\n",
        "\n",
        "You can find an example of how shared memory works below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8AF358WAFE0"
      },
      "source": [
        "### Demo: shared memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X0UxoWpAFE0"
      },
      "outputs": [],
      "source": [
        "def make_shared_array(shape, dtype, fill=None, lock=True):\n",
        "    \"\"\" Create a numpy array that is shared across processes. \"\"\"\n",
        "    size = int(np.prod(shape))\n",
        "    ctype = np.ctypeslib.as_ctypes_type(dtype)\n",
        "    if lock:\n",
        "        x_mp = mp.Array(ctype, size, lock=True).get_obj()\n",
        "    else:\n",
        "        x_mp = mp.Array(ctype, size, lock=False)\n",
        "    array = np.ctypeslib.as_array(x_mp)\n",
        "    if fill is not None:\n",
        "        array[...] = fill\n",
        "    return np.reshape(array, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lc775zDAFE1"
      },
      "outputs": [],
      "source": [
        "shared_array = make_shared_array((5, 5), 'float32', fill=1)\n",
        "normal_array = np.ones((5, 5), 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXishO2FAFE2"
      },
      "outputs": [],
      "source": [
        "def proc_A():\n",
        "    time.sleep(0.5)\n",
        "    print(\"A: setting value at [2, 3]\")\n",
        "    shared_array[2, 3] = 42\n",
        "    normal_array[2, 3] = 42\n",
        "    time.sleep(1)\n",
        "    print(f\"A: value after 1.5s: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")\n",
        "\n",
        "def proc_B():\n",
        "    print(f\"B: initial value: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")\n",
        "    time.sleep(1)\n",
        "    print(f\"B: value after 1s: normal = {normal_array[2, 3]}\\t shared = {shared_array[2, 3]}\")\n",
        "    print(\"B: dividing value at [2, 3] by 2\")\n",
        "    shared_array[2, 3] /= 2\n",
        "    normal_array[2, 3] /= 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81N-7Xi9AFE3"
      },
      "outputs": [],
      "source": [
        "mp.Process(target=proc_A).start()\n",
        "mp.Process(target=proc_B).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWm-b7dRAFE4"
      },
      "outputs": [],
      "source": [
        "# the same can be done with individual values:\n",
        "x = mp.Value(np.ctypeslib.as_ctypes_type(np.int32))\n",
        "x.value += 1 # shared across all processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG-du4ddAFE6"
      },
      "source": [
        "__So, let's put all trainable parameters in shared memory!__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1JB__1kAFE6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "class SharedEmbeddings:\n",
        "    \"\"\"\n",
        "    Word embeddings trainable parameters, allocated in shared memory\n",
        "    \"\"\"\n",
        "    def __init__(self, vocabulary_size: int, embedding_dimension: int, init_scale: float = 0.01):\n",
        "        self.embeddings = make_shared_array([vocabulary_size, embedding_dimension], np.float32, lock=False)\n",
        "        self.embeddings[...] = np.random.randn(*self.embeddings.shape) * init_scale\n",
        "\n",
        "        self.biases = make_shared_array([vocabulary_size], np.float32, fill=0.0, lock=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr5PV0MfAFE6"
      },
      "source": [
        "### Training (single-core baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWJrOBAAAFE7"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "max_steps = 10 ** 6\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "timestep_history = []\n",
        "loss_history = []\n",
        "\n",
        "model = SharedEmbeddings(vocabulary_size, embedding_dimension=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYwgvX5CAFE7"
      },
      "outputs": [],
      "source": [
        "for t in trange(max_steps):\n",
        "    batch_ix = np.random.randint(0, len(cooc.row), size=batch_size)\n",
        "    ii, jj, counts_ij = cooc.row[batch_ix], cooc.col[batch_ix], cooc.data[batch_ix]\n",
        "\n",
        "    # Compute gradients\n",
        "    emb_ii, emb_jj, bias_ii, bias_jj = \\\n",
        "      model.embeddings[ii], model.embeddings[jj], model.biases[ii], model.biases[jj]\n",
        "\n",
        "    grad_emb_ii, grad_emb_jj, grad_bias_ii, grad_bias_jj = compute_grads(\n",
        "        emb_ii, emb_jj, bias_ii, bias_jj, counts_ij)\n",
        "\n",
        "    # SGD step\n",
        "    model.embeddings[ii] -= learning_rate * grad_emb_ii\n",
        "    model.embeddings[jj] -= learning_rate * grad_emb_jj\n",
        "    model.biases[ii] -= learning_rate * grad_bias_ii\n",
        "    model.biases[jj] -= learning_rate * grad_bias_jj\n",
        "\n",
        "    if t % 10_000 == 0:\n",
        "        batch_ix = np.random.randint(0, len(cooc.row), size=4096)\n",
        "        ii, jj, counts_ij = cooc.row[batch_ix], cooc.col[batch_ix], cooc.data[batch_ix]\n",
        "        emb_ii, emb_jj, bias_ii, bias_jj = \\\n",
        "            model.embeddings[ii], model.embeddings[jj], model.biases[ii], model.biases[jj]\n",
        "\n",
        "        timestep_history.append(time.perf_counter() - start_time)\n",
        "        loss_history.append(compute_loss(emb_ii, emb_jj, bias_ii, bias_jj, counts_ij))\n",
        "        clear_output(True)\n",
        "        plt.plot(timestep_history, loss_history)\n",
        "        plt.xlabel('training time(seconds)')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noZiGvffAFE8"
      },
      "source": [
        "__Now let's parallelize it!__\n",
        "\n",
        "The code above is cute, but it only uses one CPU core. Surely we can go faster!\n",
        "\n",
        "The main challenge in this week's seminar is to speed up GloVe training by all means necessary.\n",
        "\n",
        "Here's what you should do:\n",
        "* Make multiple parallel workers, each training your model on different random data,\n",
        "* Build some centralized means of progress tracking: track the average loss and the number of training steps,\n",
        "* Implement workers in such a way that no process is left hanging after the training is over.\n",
        "\n",
        "\n",
        "Finally, please compare the loss / training time plot of your algorithm against the baseline.\n",
        "You need to run full training both for the baseline and for the parallelized version.\n",
        "\n",
        "_Notes:_\n",
        "* Remember to set a different np.random.seed in each worker!\n",
        "* You can track the training progress either via mp.Pipe or via shared variables\n",
        "* It is better to separate training and plotting into different processes\n",
        "* If you want to prevent concurrent updates to shared memory, you can use [mp.Lock](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Lock) or similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTD4WQtjAFE9"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "max_steps = 10 ** 6\n",
        "start_time = time.perf_counter()\n",
        "timestep_history = []\n",
        "loss_history = []\n",
        "\n",
        "model = SharedEmbeddings(vocabulary_size, embedding_dimension=256)\n",
        "\n",
        "\n",
        "# <YOUR CODE HERE> - optional preparations, auxiliary functions, locks, pipes, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtS6YHzmAFE9"
      },
      "outputs": [],
      "source": [
        "# <YOUR CODE HERE> - actually train the model, track performance, and clean up at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHqe0znbxVZ8"
      },
      "source": [
        "### Final boss: train GloVe without shared arrays (2 points)\n",
        "\n",
        "If you are reading this, you just wrote an asynchronous multiprocessing trainer for GloVe (congrats!).\n",
        "For the final stretch of this notebook, we'll upgrade the algorithm from multi-process to multi-server â€” simulated for now.\n",
        "The main difficulty of multu-server code is that you can no longer use shared memory.\n",
        "This is because each subprogram runs on a different node that has its own physical RAM.\n",
        "\n",
        "Thus, your goal is to rewrite the training code so that it **does not use shared memory** (make_shared_array, mp.Value or similar).\n",
        "If two processes need to communicate, they need to do that by sending data over a pipe or similar structure, not by writing to a shared array.\n",
        "\n",
        "Everything else (pipes, queues, etc.) are fair game: in real distributed systems, you can use tcp sockets for pipes, and dedicated systems for queues (zeroMQ/rabbitMQ/kafka/etc.) - but feel free to use regular pipes/queues for this assignment.\n",
        "Note that this (simulated) system will likely be somewhat slower than using shared memory, which is okay for this task.\n",
        "\n",
        "Below, we propose an **optional interface** that you can use to implement this kind of system.\n",
        "If you do not want to follow it, feel free to discard the interface and write whatever system you want.\n",
        "\n",
        "\n",
        "__If you chose not to follow the interface, please delete it from the notebook to speed up homework grading!__\n",
        "However, you will need to document the system you implemented: as a minimum, each method/class should have a docstring explaining its purpose and interface (i.e., arguments and return values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "is_executing": true,
        "id": "gRemFA_lxVZ9"
      },
      "outputs": [],
      "source": [
        "class ParameterServer(mp.Process):\n",
        "    \"\"\" A parameter server running in a background process. See the usage example below.\"\"\"\n",
        "\n",
        "    def __init__(self, vocabulary_size: int, embedding_dimension: int, init_scale: float = 0.01):\n",
        "        self.embeddings_shape = vocabulary_size, embedding_dimension\n",
        "        self.init_scale = init_scale\n",
        "\n",
        "        self.request_receiver, self.request_sender = mp.Pipe(duplex=False) # used for processing requests\n",
        "        super().__init__(name=self.__class__.__name__, daemon=True)  # daemons die if the host is killed\n",
        "        # see also: https://docs.python.org/2/library/multiprocessing.html#multiprocessing.Process.daemon\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"This method is called inside the process after you call .start()\"\"\"\n",
        "        self.embeddings = np.random.randn(*self.embeddings_shape).astype(np.float32) * self.init_scale\n",
        "        self.biases = np.zeros([len(self.embeddings)], dtype=np.float32)\n",
        "\n",
        "        while True:\n",
        "            # receive a request from another process\n",
        "            name, args, output_pipe = self.request_receiver.recv()\n",
        "\n",
        "            call_method_by_name = getattr(self, name)\n",
        "            result = call_method_by_name(*args)\n",
        "            output_pipe.send(result)\n",
        "\n",
        "\n",
        "    def example_add(self, a, b):\n",
        "        \"\"\"An example method that adds two vectors inside a server\"\"\"\n",
        "        print(f\"Requesting to run _example_add (i am pid={os.getpid()})\")\n",
        "\n",
        "        return_pipe, output_pipe = mp.Pipe() # a pipe that will be used to send result back to you\n",
        "        self.request_sender.send((\"_example_add\", (a, b), output_pipe))\n",
        "        #                         method name,    args,   where to send result\n",
        "        result = return_pipe.recv()\n",
        "        print(f\"Received results (i am pid={os.getpid()})\")\n",
        "        return result\n",
        "\n",
        "    def _example_add(self, a, b):\n",
        "        print(f\"Running _example_add from the background process! (i am pid={os.getpid()})\")\n",
        "        return a + b\n",
        "\n",
        "    def get_embeddings(self, indices):\n",
        "        \"\"\"\n",
        "        Return latest parameters (weights and biases)\n",
        "        This method is meant to be called by other processes.\n",
        "\n",
        "        :param indices: a numpy array with word indices (integers)\n",
        "        :returns: embeddings and biases for the specified indies only\n",
        "        \"\"\"\n",
        "        return_pipe, output_pipe = mp.Pipe()\n",
        "        self.request_sender.send((\"_get_embeddings\", (indices,), output_pipe))\n",
        "        return return_pipe.recv()\n",
        "\n",
        "    def _get_embeddings(self, indices):\n",
        "        return self.embeddings[indices], self.biases[indices]\n",
        "\n",
        "    def update_embeddings(self, indices, embeddings_update, biases_update):\n",
        "        \"\"\"Ask the server to update a portion of their embeddings and biases\"\"\"\n",
        "        raise NotImplementedError(\"\"\"TODO. When implementing this, you will need to write an additional function\"\"\")\n",
        "\n",
        "    def finish_training(self):\n",
        "        raise NotImplementedError(\"\"\" TODO (optional). Get the final trained embeddings and stop the process\"\"\")\n",
        "\n",
        "    # (feel free to add any extra functions you may want)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrD6urouxVZ9"
      },
      "source": [
        "### Example: using the ParameterServer process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DiyipDtxVZ9"
      },
      "outputs": [],
      "source": [
        "parameter_server = ParameterServer(123, 100)\n",
        "parameter_server.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "680rI7nfxVZ-"
      },
      "outputs": [],
      "source": [
        "# ask PS to add two arrays using _example_add\n",
        "parameter_server.example_add(np.ones(3), np.arange(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmmIzRFDxVZ-"
      },
      "outputs": [],
      "source": [
        "ii = np.arange(5, 10)   # word indices for minibatch SGD\n",
        "embeddings_ii, biases_ii = parameter_server.get_embeddings(ii)\n",
        "\n",
        "print(f\"Received embeddings of shape {embeddings_ii.shape}:\\n\", repr(embeddings_ii)[:300], '...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bWIZH4sxVZ-"
      },
      "outputs": [],
      "source": [
        "parameter_server.terminate()   # a less-than-nice way to kill a process\n",
        "\n",
        "# note: after you kill a server, it no longer responds to your requests (duh)\n",
        "# if your remote function causes an exception, it will also kill the process\n",
        "\n",
        "# If you've broken a parameter server, just restart it and pretend like nothing happened!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za3zoxbuxVZ-"
      },
      "source": [
        "### Your code here :)\n",
        "\n",
        "If you're feeling lost, there are optional tips on how to proceed at the bottom of the notebook.\n",
        "\n",
        "When you're done, please write a short informal description of what you've done, what worked, and what didn't."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGy1tyBkxVZ_"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "max_steps = 10 ** 6\n",
        "start_time = time.perf_counter()\n",
        "timestep_history = []\n",
        "loss_history = []\n",
        "\n",
        "model = ParameterServer(vocabulary_size, embedding_dimension=256)\n",
        "\n",
        "# <YOUR CODE HERE> - optional preparations, auxiliary functions, locks, pipes, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZvtV-KzxVZ_"
      },
      "outputs": [],
      "source": [
        "# <YOUR CODE HERE> - actually train the model, track performance, and clean up at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME0lZHuKAFE-"
      },
      "source": [
        "__Describe your solution__\n",
        "\n",
        "Hello, I'm `______` and here's what i've done:\n",
        "\n",
        "* something\n",
        "\n",
        "\n",
        "(please include the full description, including the monitoring/testing steps you've done in the previous assignment, when using the shared memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu4BkOjxVaA"
      },
      "source": [
        "```\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuUZ7Pm3xVaA"
      },
      "source": [
        "__Optional tips for this assignemnt__\n",
        "\n",
        "1. Implement a method for sending updates (delta weights) to the server\n",
        "2. Test that this method works, similarly to the examples above\n",
        "  - After you update the embeddings, it is reasonable to expect that .get_embeddings returns new values\n",
        "  \n",
        "3. Run a training with a single worker, using `.get_embeddings` and `.update_embeddings`\n",
        "  - Use your solution from the previous task for inspiration\n",
        "  \n",
        "4. Add multiple workers and try to achieve speed-up v.s. a single worker\n",
        "  - Likewise, your previous solution may come in handy\n",
        "\n",
        "5. Realize that the trained embeddings are lost to you, forever left in a different process\n",
        "  - Implement code that gathers final embeddings in the main process\n",
        "  - Add loss monitoring / learning curves, similar to the previous assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sYVc0kdxVaA"
      },
      "source": [
        "### Bonus assignment: staleness-aware SGD (2 extra points)\n",
        "\n",
        "If you want to practice more with asynchronous training, try to implement staleness-aware SGD with learning rate correction.\n",
        "You can find a description of this algorithm [here](https://arxiv.org/abs/1511.05950), or in the lecture slides.\n",
        "\n",
        "To better test asynchronous SGD correction on one machine, you can simulate what happens with distributed training by adding a simulated latency to your workers.\n",
        "To do so, simply `time.sleep` for a random duration before sending each update to the server.\n",
        "\n",
        "Note that GloVe is more resilient to staleness than other models (e.g. ResNets or transformers) because the updates are sparse.\n",
        "To observe a substantial slowdown from staleness, you need to either train with many workers or increase the batch size so that the updates intersect more often.\n",
        "\n",
        "To get the full grade for this bonus assignment, try to find a regime where staleness-aware SGD brings observable wall-clock speed-upds and compare the learning curves between all three methods for training GloVe."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}